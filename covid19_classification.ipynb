import os
import cv2
import glob
import torch 
import torchvision
import numpy as np
import torch.nn as nn
from google.colab import drive
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
print("Imported ! ")
#---------------------------------------------
# Setting up the Training data transformations
#---------------------------------------------

def get_train_image_transform():
    w=256
    h=256
    train_transform = transforms.Compose(
                        [transforms.RandomHorizontalFlip(),
                        transforms.ToTensor(),
                        transforms.Resize((w, h)),
                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    return train_transform

#--------------------------------------------
# Setting up the Testing data transformations
#--------------------------------------------

def get_test_image_transform():
    w=256
    h=256
    test_transform = transforms.Compose(
                        [transforms.ToTensor(),
                        transforms.Resize((w, h)),
                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    return test_transform

#----------------------------------
# Function to get the images paths 
#----------------------------------

def get_image_paths(data_path, categories):

    train_image_paths, train_labels = [], []
    for cat in os.listdir(data_path+'train/'):
        imgs = glob.glob(data_path+'train/'+cat+'/*.*')
        train_image_paths = train_image_paths + imgs
        train_labels = train_labels + [cat]*len(imgs)

    test_image_paths, test_labels = [], []
    for cat in os.listdir(data_path+'test/'):
        imgs = glob.glob(data_path+'test/'+cat+'/*.*')
        test_image_paths = test_image_paths + imgs
        test_labels = test_labels + [cat]*len(imgs)

    valid_image_paths, valid_labels = [], []
    for cat in os.listdir(data_path+'validation/'):
        imgs = glob.glob(data_path+'validation/'+cat+'/*.*')
        valid_image_paths = valid_image_paths + imgs
        test_labels = valid_labels + [cat]*len(imgs)

    return np.array(train_image_paths), np.array(test_image_paths), np.array(valid_image_paths), np.array(train_labels), np.array(test_labels) , np.array(valid_labels) 

    

#----------------------------
# Accessing the google drive 
#----------------------------

drive.mount("./gdrive")
%cd /gdrive

#----------------------------
# Defining the Data set class 
#----------------------------

class custom_dataset(Dataset):

    def __init__(self, images, labels, transforms=True):
        self.images=images
        self.labels=labels
        self.transforms=transforms
       

    def __len__(self):
        return len(self.images)
      
    def __getitem__(self, index):
        if self.transforms is True:
            self.images = self.transforms(self.images)
            self.labels=self.transforms(self.labels)
            
        return self.images[index], self.labels[index]

#-----------------------------------------------------------------------------------------------------------------------
# Defining the Data path and calling the path function to get paths for the training, testing and validation images data
#-----------------------------------------------------------------------------------------------------------------------
#data_path ='C:/Users/Iqra/Desktop/Iqra_msee19024_05/Assignment_5_Dataset/'
data_path ='./gdrive/My Drive/Colab Notebooks/Assignment 5 Dataset/'
categories=np.array(['infected', 'normal'])
train_images, test_images, validation_image, train_labels, test_labels, validation_labels = get_image_paths(data_path, categories)
print("The training data size is : ", np.size(train_images), "\n")
train_transformations=get_train_image_transform()
print("Transformation for training data are acquired ! \n")

#--------------------------------------------
# Defining the training dataset class objects 
#--------------------------------------------
train_set=custom_dataset(train_images, train_labels,transforms=train_transformations )
print("Transformations have been applied on the provided images and labels", "\n")
'''
first_data=train_set[0]
feat, lab=first_data
#print(feat, lab)
'''
#------------------------------------------
# Defining the training data loader object
#------------------------------------------
train_loader = DataLoader(train_set,batch_size=16, shuffle=True)
#train_loader = DataLoader(train_set, batch_size=24, shuffle=True)

#---------------------------------------------------
# Defining the iterator of the training data loader
#---------------------------------------------------
train_iterator=iter(train_loader)

#-------------------------------------------
# Reading the data batch wise using iterator
#-------------------------------------------

batch=next(train_iterator)
'''
for iterations in range(10):
    trainX, trainY=next(train_iterator)
'''
#----------------------------------------------
# Testing data size and testing transformations
#----------------------------------------------    

print("The testing data size is : ", test_images.size, "\n")
test_transformations=get_test_image_transform()
print("Transformation for testing data are acquired ! \n")

#--------------------------------------------
# Defining the testing dataset class objects 
#--------------------------------------------
test_set=custom_dataset(test_images, test_labels,transforms=test_transformations )
print("Transformations have been applied on the provided images and labels", "\n")
'''
first_data=train_set[0]
feat, lab=first_data
#print(feat, lab)
'''
#------------------------------------------
# Defining the training data loader object
#------------------------------------------
test_loader=DataLoader(test_set,batch_size=1, shuffle=True)

#---------------------------------------------------
# Defining the iterator of the training data loader
#---------------------------------------------------
test_iterator=iter(test_loader)

#-------------------------------------------
# Reading the data batch wise using iterator
#-------------------------------------------

batch=next(test_iterator)
'''
for iterations in range(2):
    trainXt, trainYt=next(test_iterator)
'''

#-----------------------
# Defining Classes tuple
#-----------------------

classes=("infected","normal")

#-----------------------------------------
# Checking the training data using imshow
#-----------------------------------------
'''
trainItr = iter(test_loader)
img, label = trainItr.next()
print(img.shape) #(Batch, Channels, Width, Height)
print(label)
'''

#---------------------------------------------------------------------------------
# Building a Convolutional Neural Network Class using nn Module from torch library
#---------------------------------------------------------------------------------

import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()

        self.CNN_layers = nn.Sequential(
            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=1), #input_size = 3x32x32 -->> output_size = 6x28x28
            nn.ReLU(),     #input_size = 6x28x28 -->> output_size = 6x28x28
            nn.MaxPool2d(kernel_size=2, stride=2),   #input_size = 6x28x28 -->> output_size = 6x14x14
            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=1), #input_size = 6x14x14 -->> output_size = 16x10x10
            nn.ReLU(), #input_size = 16x10x10 -->> output_size = 16x10x10
            nn.MaxPool2d(kernel_size=2, stride=2), #input_size = 16x10x10 -->> output_size = 16x5x5  = 400x1
        )
        self.fc = nn.Sequential(
            nn.Linear(16 * 5 * 5, 120), #(no of inputs, no of outputs)
            nn.ReLU(),
            nn.Linear(120, 48),
            nn.ReLU(),
            nn.Linear(48, 10),
        )
        return
    
    def forward(self, image_features):
        #passing the image features to the Convolutional Layer of the CNN
        image_features = self.CNN_layers(image_features)

        #Flattening the resultant layer
        image_features = image_features.view(-1, 16 * 5 * 5) 

        ##passing the flattened image features to the Fully Connected layer of the CNN
        image_features = self.fc(image_features)
        
        return image_features


#---------------------------------------------------------------------------
# Creating the Convolutional Neural Network Object using above defined class
#---------------------------------------------------------------------------

net=Net()
print("The network layers are as follow: \n",net,'\n')

#--------------------------------- ------------
# Defining the loss function and the optimizer
#----------------------------------------------

learning_rate = 0.001
epochs = 10

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
print("Using Cross Entropy Loss function and Adam Optimizer \n")

#--------------------------------------------
# Defining the training model for the layers
#--------------------------------------------

from tqdm import tqdm

#if you have gpu then you need to convert the network and data to cuda
#the easiest way is to first check for device and then convert network and data to device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net.to(device)

net.train()
# net.eval()

for epoch in range(epochs):  

    running_loss = 0.0
    pbar = tqdm(enumerate(train_loader))
    for i, data in pbar:
        # get the inputs
        inputs, labels = data
        
        #inputs, labels= np.array(inputs), np.array(labels)
        print(inputs)
        print(labels)
        #print(torch.from_numpy(inputs))
        #print(torch.tensor([inputs]))
        inputs = inputs.to(device)
        labels = labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()
        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation 
        # because PyTorch accumulates the gradients on subsequent backward passes. 
        # This is convenient while training RNNs. 
        # So, the default action is to accumulate the gradients on every loss.backward() call

        # forward + backward + optimize
        outputs = net(inputs)               #----> forward pass
        loss = criterion(outputs, labels)   #----> compute loss
        loss.backward()                     #----> backward pass
        optimizer.step()                    #----> weights update

        # print statistics
        running_loss += loss.item()
        
        pbar.set_description(
            'Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, i * len(inputs), len(train_loader.dataset),
                100. * i / len(train_loader),
                loss.data))
        
    #save a PyTorch model
    torch.save(net.state_dict(), 'trained.pth')

print('Finished Training')



#-----------------------------------------------------
# Loading the RESNET-18 pretrained model from py-torch
#-----------------------------------------------------


resnet18 = models.resnet18(pretrained=True, progress=True)


transform = transforms.Compose(
                        [transforms.RandomHorizontalFlip(),
                        transforms.ToTensor(),
                        transforms.Resize((256, 256)),
                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

dataset = datasets.ImageNet(".", split="train", transform=transform)

#---------------------------------
# Overall Accuracy of the network
#---------------------------------
correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))

#-----------------------------------
# Class-wise Accuracy of the network
#-----------------------------------

class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in test_loader:
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(1):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1


for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))
